# -*- coding: utf-8 -*-
"""Qwen-streamlit-coder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10_F-iDfUW2KT-rQlVsf3Ytp8ntcFSobu
"""

# Cell 1: Setup and Installations
!pip install transformers datasets accelerate bitsandbytes
!pip install huggingface_hub
!pip install torch torchvision torchaudio

# Logging in to Hugging Face 
from huggingface_hub import login
login()  # This will prompt for your HF token (ps. you will need a hf account and should generate your own key)

#Importing libraries
import torch
import json
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from huggingface_hub import HfApi
import os

# lets check if GPU is available, for this tutorial colab free gpu units are enough
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# # Cell 4: Gemini API Setup and Synthetic Data Generation
# import google.generativeai as genai
# from google.colab import userdata
# import time
# import random

# # Configure Gemini API
# GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')  # Make sure this matches your secret name
# genai.configure(api_key=GEMINI_API_KEY)

# # Initialize the model
# model = genai.GenerativeModel('gemini-2.0-flash-exp')

# class SyntheticDataGenerator:
#     """
#     Generate synthetic Streamlit training data using Gemini
#     """

#     def __init__(self):
#         self.categories = [
#             "finance", "healthcare", "education", "entertainment",
#             "travel", "food", "technology", "automotive", "fitness", "productivity"
#         ]

#         self.complexities = ["simple", "medium", "complex"]

#         self.app_types = [
#             "dashboard", "tracker", "analyzer", "calculator",
#             "planner", "manager", "monitor", "generator"
#         ]

#     def create_data_generation_prompt(self, category, complexity, app_type):
#         """
#         Create a prompt for Gemini to generate Streamlit code
#         """

#         prompt = f"""You are an expert Streamlit developer. Generate a complete, working Streamlit application.

# Requirements:
# - Category: {category}
# - Complexity: {complexity}
# - App Type: {app_type}
# - Must use modern Streamlit components
# - Include proper imports
# - Add realistic sample data
# - Use appropriate charts/visualizations

# Complexity Guidelines:
# - Simple: 20-40 lines, basic components
# - Medium: 40-80 lines, multiple sections, some interactivity
# - Complex: 80-150 lines, multiple pages, advanced features

# Generate ONLY the Python code, no explanations or markdown formatting.
# The code should be production-ready and follow Streamlit best practices."""

#         return prompt

#     def generate_single_example(self, category, complexity, app_type):
#         """
#         Generate one training example using Gemini
#         """
#         try:
#             # Create the prompt
#             code_prompt = self.create_data_generation_prompt(category, complexity, app_type)

#             # Generate code using Gemini
#             response = model.generate_content(code_prompt)
#             generated_code = response.text

#             # Create instruction for the training data
#             instruction = f"Create a {complexity} {category} {app_type} using Streamlit"

#             # Create input context
#             input_context = f"App category: {category}, Complexity: {complexity}, Type: {app_type}"

#             return {
#                 "instruction": instruction,
#                 "input": input_context,
#                 "output": generated_code.strip()
#             }

#         except Exception as e:
#             print(f"Error generating example: {e}")
#             return None

#     def generate_training_dataset(self, num_examples=50):
#         """
#         Generate multiple training examples
#         """
#         print(f"Generating {num_examples} training examples...")
#         examples = []

#         for i in range(num_examples):
#             # Random combination
#             category = random.choice(self.categories)
#             complexity = random.choice(self.complexities)
#             app_type = random.choice(self.app_types)

#             print(f"Generating example {i+1}/{num_examples}: {category} {complexity} {app_type}")

#             example = self.generate_single_example(category, complexity, app_type)

#             if example:
#                 examples.append(example)

#             # Small delay to avoid rate limiting
#             time.sleep(1)

#         print(f"Successfully generated {len(examples)} examples")
#         return examples

# # Test with a small batch first
# generator = SyntheticDataGenerator()

# # Generate just 3 examples first to test
# print("Testing with 3 examples...")
# test_examples = generator.generate_training_dataset(num_examples=3)

# # Display first example
# if test_examples:
#     print("\n" + "="*50)
#     print("FIRST GENERATED EXAMPLE:")
#     print("="*50)
#     print(f"Instruction: {test_examples[0]['instruction']}")
#     print(f"Input: {test_examples[0]['input']}")
#     print(f"Output (first 300 chars): {test_examples[0]['output'][:300]}...")

# # Cell 5: Clean and Format the Generated Data
# import re
# import json

# class DataFormatter:
#     """
#     Clean and format the Gemini-generated examples for training
#     """

#     def clean_code_output(self, code_text):
#         """
#         Remove markdown formatting and clean the code
#         """
#         # Remove ```python and ``` markers
#         cleaned = re.sub(r'```python\n?', '', code_text)
#         cleaned = re.sub(r'```\n?', '', cleaned)

#         # Remove any extra whitespace
#         cleaned = cleaned.strip()

#         return cleaned

#     def format_for_training(self, examples):
#         """
#         Format examples for instruction-following training
#         """
#         formatted_examples = []

#         for example in examples:
#             # Clean the code output
#             cleaned_code = self.clean_code_output(example['output'])

#             # Create the training format
#             training_text = f"""### Instruction:
# {example['instruction']}

# ### Input:
# {example['input']}

# ### Response:
# {cleaned_code}"""

#             formatted_examples.append({
#                 "text": training_text
#             })

#         return formatted_examples

# # Clean the test examples
# formatter = DataFormatter()

# # Clean and format the 2 successful examples
# cleaned_examples = formatter.format_for_training(test_examples)

# print("Cleaned and formatted examples:")
# print(f"Number of examples: {len(cleaned_examples)}")
# print("\n" + "="*50)
# print("CLEANED EXAMPLE:")
# print("="*50)
# print(cleaned_examples[0]["text"][:600] + "...")
# now that we have seen how data should look like(prompt:, resonse: format) for SFT we will generae a synthetic data for finetuning a coding model based on generating quality streamlit code


# # Cell 6: Generate Full Training Dataset (for this tutorial i have kept 50 rows, the more "correct labels" the better)
# import json
# from datetime import datetime

# def generate_full_dataset(num_examples=50):
#     """
#     Generate a full training dataset and save it
#     """
#     print(f"Generating {num_examples} training examples...")

#     # Generate raw examples using Gemini
#     raw_examples = generator.generate_training_dataset(num_examples=num_examples)

#     # Clean and format for training
#     formatted_examples = formatter.format_for_training(raw_examples)

#     # Save the dataset
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = f"streamlit_training_data_{timestamp}.json"

#     with open(filename, 'w') as f:
#         json.dump(formatted_examples, f, indent=2)

#     print(f"\nDataset saved as: {filename}")
#     print(f"Total examples: {len(formatted_examples)}")

#     # Show statistics
#     show_dataset_stats(formatted_examples)

#     return formatted_examples, filename

# def show_dataset_stats(examples):
#     """
#     Show statistics about the generated dataset
#     """
#     total_chars = sum(len(ex["text"]) for ex in examples)
#     avg_length = total_chars / len(examples) if examples else 0

#     print(f"\n Dataset Statistics:")
#     print(f"   Total examples: {len(examples)}")
#     print(f"   Average example length: {avg_length:.0f} characters")
#     print(f"   Total dataset size: {total_chars:,} characters")

# # Generate the full dataset
# # Start with 50 examples to test (you can increase later)
# full_dataset, dataset_file = generate_full_dataset(num_examples=50)

# Cell 8: Load Dataset and Prepare for Training
import json
from datasets import Dataset

#  Load the generated dataset from local session ( i will upload my data on github you can download it from there, replace the path and run it)
filename = '/content/streamlit_training_data_20250615_203751.json'  # Update to your own file path if different
with open(filename, 'r') as f:
    training_data = json.load(f)

print(f"Loaded {len(training_data)} training examples from local session")

# Convert to Hugging Face dataset format
dataset = Dataset.from_list(training_data)
print(f"Created Hugging Face dataset with {len(dataset)} examples")

# Split into train/validation (80/20 split)
train_test_split = dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']

print(f"Training examples: {len(train_dataset)}")
print(f"Validation examples: {len(eval_dataset)}")

# Show first example
print("\nFirst training example:")
print(train_dataset[0]['text'][:400] + "...") # just looking at the start of the streamlit code to make sure it is right

# Show dataset info
print(f"\nDataset Statistics:")
total_chars = sum(len(example['text']) for example in training_data)
avg_length = total_chars / len(training_data)
print(f"    Average example length: {avg_length:.0f} characters")
print(f"    Total dataset size: {total_chars:,} characters")

# Cell 9: Load Qwen2.5-Coder (for whatever task you fine tune, use the model for which it is best, for example here my aim is code generation so i have used coding model for best results)
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
import torch

MODEL_NAME = "Qwen/Qwen2.5-Coder-1.5B-Instruct"

print(f" Loading {MODEL_NAME}...")
print("This model is specifically designed for code generation!")

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

print(f"Model loaded successfully!")
print(f"Model parameters: {model.num_parameters():,}")

# cell 10: Just to test the base model with proper Qwen chat format
def test_qwen_coder(prompt):
    messages = [
        {"role": "system", "content": "You are Qwen, a helpful coding assistant. Generate clean, working code."},
        {"role": "user", "content": f"Create a Streamlit application: {prompt}"}
    ]

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    with torch.no_grad():
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=400,
            temperature=0.3,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

print("\nTesting Qwen2.5-Coder base model:")
print("="*60)
result = test_qwen_coder("simple finance dashboard with expense tracking and charts")
print(result)

# cell 10 Since we are using free colab units its useful to keep track of our GPU utilization, the next block will do a quick memory check
print(f" Current GPU memory usage:")
if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated(0) / 1e9
    total = torch.cuda.get_device_properties(0).total_memory / 1e9
    free = total - allocated
    print(f"   Total: {total:.1f} GB")
    print(f"   Used: {allocated:.1f} GB")
    print(f"   Free: {free:.1f} GB")

# Cell 11: Prepare Qwen2.5-Coder for Fine-tuning (before we move forward we need to make sure our data is prepared for training in the way qwen expects it)
# Tokenization function for Qwen chat format
def tokenize_qwen_format(examples):
    formatted_texts = []

    for text in examples["text"]:
        # Extract instruction and code from our training format
        parts = text.split("### Response:")
        if len(parts) == 2:
            instruction = parts[0].replace("### Instruction:", "").replace("### Input:", "").strip()
            code = parts[1].strip()

            # Format as Qwen chat
            messages = [
                {"role": "system", "content": "You are Qwen, a helpful coding assistant. Generate clean, working Streamlit code."},
                {"role": "user", "content": f"Create a Streamlit application: {instruction}"},
                {"role": "assistant", "content": code}
            ]

            # Apply chat template
            formatted_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
            formatted_texts.append(formatted_text)

    # Tokenize
    return tokenizer(
        formatted_texts,
        truncation=True,
        padding=False,
        max_length=1024
    )

# Tokenize datasets, make sure you tokenize with the correct model name passed in MODEL_NAME, different models use different tokenizers 
print("Tokenizing datasets for Qwen format...")
tokenized_train = train_dataset.map(tokenize_qwen_format, batched=True, remove_columns=["text"])
tokenized_eval = eval_dataset.map(tokenize_qwen_format, batched=True, remove_columns=["text"])

print(f"Tokenization complete!")
print(f"Training examples: {len(tokenized_train)}")
print(f"Validation examples: {len(tokenized_eval)}")

# Cell 12: Complete Memory Cleanup, If you have a lot of GPU (more that free units no need to run this block)
# import torch
# import gc
# import os

# # Set memory optimization
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# # Complete cleanup
# torch.cuda.empty_cache()

# # Delete everything
# try:
#     del model
#     del trainer
#     del tokenizer
#     del minimal_lora_config
#     del minimal_args
#     del minimal_collator
# except:
#     pass

# gc.collect()
# torch.cuda.empty_cache()

# print("Complete memory cleanup done!")

# cell 13 : Check available memory
if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated(0) / 1e9
    total = torch.cuda.get_device_properties(0).total_memory / 1e9
    free = total - allocated
    print(f"Free GPU memory: {free:.1f} GB")
    print(f"Total GPU memory: {total:.1f} GB")

# Cell 14: Load with Proper 4-bit Configuration since the model is heavy we use bits and bytes config and load it in 4 bit configuration
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

print("Loading Qwen2.5-Coder with proper 4-bit quantization...")

# Proper 4-bit quantization config
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,  # Faster computation
    bnb_4bit_quant_type="nf4",            # Better quantization
    bnb_4bit_use_double_quant=True,       # More memory savings
)

# Loading the model
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-Coder-1.5B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16,
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-Coder-1.5B-Instruct")

print("Model loaded with proper 4-bit quantization!")

# cell 15: Check memory usage (since i am using free units its better to keep in check)
if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated(0) / 1e9
    print(f"GPU memory used: {allocated:.1f} GB")

# Cell 16:  LoRA Setup uses these libraries
from peft import LoraConfig, get_peft_model, TaskType

print("Setting up enhanced LoRA configuration...")

# LoRA config for code generation
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=4,                             # decent capacity for code structure
    lora_alpha=16,                   # Balances adaptation and stability
    lora_dropout=0.1,                # Helps with generalization (especially for 50 rows)
    target_modules=[                 # Expanded for both attention + MLP layers, has more modules read online for what each does (transformers primarily uses key, value and query)
        "q_proj", "k_proj", "v_proj"
    ],
)

# Apply LoRA to model to decompose rank and track weight
model = get_peft_model(model, lora_config)

# Print trainable parameters
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())

print(f"LoRA applied successfully!")
print(f"Trainable parameters: {trainable_params:,}")
print(f"Percentage trainable: {trainable_params / total_params * 100:.3f}%")

# GPU memory checking again
if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated(0) / 1e9
    print(f"GPU memory after LoRA: {allocated:.1f} GB")

# Cell 16: Training Arguments
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

print("Setting up the training config:")

# training arguments ( ultra-minimal)
training_args = TrainingArguments(
    output_dir="./qwen-improved-lora",

    # Training parameters - increase for better learning of your data
    num_train_epochs=2,            # 2, should ideally be more to learn since dataset is small based on the loss (increase to reduce loss, do not increase if you are on free units)
    per_device_train_batch_size=2, # Batch size 2 The bigger, the better for stability values are like 2,4,8,16,32
    gradient_accumulation_steps=4,  #  Allows training with larger batch sizes without running out of memory

    # Learning rate
    learning_rate=8e-4,            # Slightly higher learning rate
    warmup_steps=5,                   #Prevents training from diverging in the beginning

    # Evaluation and saving
    eval_strategy="epoch",         # Enable evaluation
    save_strategy="epoch",
    save_total_limit=1,
    load_best_model_at_end=True,

    # Logging
    logging_steps=5,
    report_to=[],   # we are not reporting (logging to tensorboard or wandb right now)

    # Memory settings
    dataloader_pin_memory=False,
    remove_unused_columns=False,
    dataloader_num_workers=0,
)

# Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
    pad_to_multiple_of=8,
)

print("Training configuration ready!")
print(f"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
print(f"total epochs: {training_args.num_train_epochs}")
print(f" Will train on all {len(tokenized_train)} examples")

# Cell 17: LoRA Training
print("Initializing improved LoRA training...")

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,  # All 38 examples
    eval_dataset=tokenized_eval,    # All 10 validation examples
    data_collator=data_collator,
    processing_class=tokenizer,
)

print(f"Training {trainable_params:,} parameters")
print(f"Training examples: {len(tokenized_train)}")
print(f"Validation examples: {len(tokenized_eval)}")
print("Estimated time: 3-5 minutes")
print("\nStarting training...")

# Start training
try:
    training_output = trainer.train()
    print("\n Training completed successfully!")

    # Get training metrics from trainer state instead
    final_train_loss = trainer.state.log_history[-1].get('train_loss', 'N/A')
    final_eval_loss = trainer.state.log_history[-1].get('eval_loss', 'N/A')

    print(f" Final train loss: {final_train_loss}")
    print(f"final eval loss: {final_eval_loss}")

    # Save the model
    print("Saving improved LoRA model...")
    model.save_pretrained("./qwen-improved-final")
    tokenizer.save_pretrained("./qwen-improved-final")
    print("Model saved successfully!")

    # Print training summary
    print(f"\nTraining Summary:")
    print(f"   Loss improved from 0.784 → {final_train_loss}")
    print(f"   Validation loss: {final_eval_loss}")
    print(f"   Trained on {len(tokenized_train)} examples for 2 epochs") #ideal loss should be <2.0 to achieve this change parameters but only if you have more GPU than free units provided by google

except Exception as e:
    print(f"Training failed: {e}")

# Cell 19: now lets Evaluate our model with BLEU/ROUGE scores (you can definetly use libraries like nltk and rouge_scorer to have a more straigtforward evaluation rather than using the following evaluation technique)
import ast
import re
from collections import Counter
import numpy as np

class CodeGenerationEvaluator:
    def __init__(self, model, tokenizer, eval_dataset):
        self.model = model
        self.tokenizer = tokenizer
        self.eval_dataset = eval_dataset

    def generate_code(self, prompt):
        """Generate code for evaluation"""
        messages = [
            {"role": "system", "content": "You are Qwen, a helpful coding assistant. Generate clean, working Streamlit code."},
            {"role": "user", "content": f"Create a Streamlit application: {prompt}"}
        ]

        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=400,
                temperature=0.3,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response

    def calculate_bleu_score(self, generated_text, reference_text):
        """Calculate BLEU score for code generation"""
        def get_ngrams(text, n):
            tokens = text.split()
            return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

        # Calculate BLEU-4 (4-gram) think of this score are precision ( basically comparing similarities between n gram)
        generated_tokens = generated_text.split()
        reference_tokens = reference_text.split()

        if len(generated_tokens) == 0:
            return 0.0

        # Calculate precision for 1-4 grams
        bleu_scores = []
        for n in range(1, 5):
            gen_ngrams = get_ngrams(generated_text, n)
            ref_ngrams = get_ngrams(reference_text, n)

            if len(gen_ngrams) == 0:
                bleu_scores.append(0.0)
                continue

            gen_counter = Counter(gen_ngrams)
            ref_counter = Counter(ref_ngrams)

            # Calculate precision
            overlap = sum((gen_counter & ref_counter).values())
            precision = overlap / len(gen_ngrams) if len(gen_ngrams) > 0 else 0
            bleu_scores.append(precision)

        # Geometric mean of precisions
        if all(score > 0 for score in bleu_scores):
            bleu = np.exp(np.mean(np.log(bleu_scores)))
        else:
            bleu = 0.0

        return bleu

    def calculate_rouge_l(self, generated_text, reference_text):
        """Calculate ROUGE-L score, think of this score as recall"""
        def lcs_length(x, y):
            m, n = len(x), len(y)
            dp = [[0] * (n + 1) for _ in range(m + 1)]

            for i in range(1, m + 1):
                for j in range(1, n + 1):
                    if x[i-1] == y[j-1]:
                        dp[i][j] = dp[i-1][j-1] + 1
                    else:
                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])
            return dp[m][n]

        gen_tokens = generated_text.split()
        ref_tokens = reference_text.split()

        if len(gen_tokens) == 0 or len(ref_tokens) == 0:
            return 0.0

        lcs_len = lcs_length(gen_tokens, ref_tokens)

        # ROUGE-L = F-measure based on LCS
        precision = lcs_len / len(gen_tokens) if len(gen_tokens) > 0 else 0
        recall = lcs_len / len(ref_tokens) if len(ref_tokens) > 0 else 0

        if precision + recall == 0:
            return 0.0

        rouge_l = 2 * precision * recall / (precision + recall)
        return rouge_l

    def calculate_exact_match(self, generated_text, reference_text):
        """Calculate exact match score,"""
        # Normalize both texts
        gen_clean = re.sub(r'\s+', ' ', generated_text.strip().lower())
        ref_clean = re.sub(r'\s+', ' ', reference_text.strip().lower())
        return 1.0 if gen_clean == ref_clean else 0.0

    def check_syntax_validity(self, code):
        """Check if generated code has valid Python syntax"""
        try:
            # Extract code from markdown if present
            code_match = re.search(r'```python\n(.*?)```', code, re.DOTALL)
            if code_match:
                code = code_match.group(1)

            ast.parse(code)
            return True
        except:
            return False

    def check_streamlit_imports(self, code):
        """Check if code imports Streamlit"""
        return "import streamlit" in code or "import st" in code

# Initialize evaluator
evaluator = CodeGenerationEvaluator(model, tokenizer, eval_dataset)
print("Code evaluation metrics initialized!")
print("Available metrics: BLEU, ROUGE-L, Exact Match, Syntax Validity, Streamlit Components")

# Cell 20: Load Saved Model and Run Complete Evaluation
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

print("Loading your fine-tuned model from /content/qwen-improved-final...")

# Load the fine-tuned model
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-Coder-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_4bit=True
)

# Load the LoRA adapters
model = PeftModel.from_pretrained(base_model, "/content/qwen-improved-final")
tokenizer = AutoTokenizer.from_pretrained("/content/qwen-improved-final")

print("Fine-tuned model loaded successfully!")

# Run comprehensive evaluation
def run_complete_evaluation():
    """Run full evaluation with all metrics"""
    results = {
        'total_examples': len(eval_dataset),
        'bleu_scores': [],
        'rouge_scores': [],
        'exact_matches': [],
        'syntax_valid': 0,
        'has_streamlit': 0,
        'detailed_results': []
    }

    print(f"Evaluating on {len(eval_dataset)} examples...")
    print("="*70)

    for i, example in enumerate(eval_dataset):
        # Extract instruction and reference code
        parts = example['text'].split("### Response:")
        if len(parts) == 2:
            instruction = parts[0].replace("### Instruction:", "").replace("### Input:", "").strip()
            reference_code = parts[1].strip()

            print(f"\nExample {i+1}/{len(eval_dataset)}")
            print(f"Instruction: {instruction[:80]}...")

            # Generate code
            generated_code = evaluator.generate_code(instruction)

            # Calculate metrics
            bleu = evaluator.calculate_bleu_score(generated_code, reference_code)
            rouge = evaluator.calculate_rouge_l(generated_code, reference_code)
            exact_match = evaluator.calculate_exact_match(generated_code, reference_code)
            syntax_valid = evaluator.check_syntax_validity(generated_code)
            has_streamlit = evaluator.check_streamlit_imports(generated_code)

            # Store results
            results['bleu_scores'].append(bleu)
            results['rouge_scores'].append(rouge)
            results['exact_matches'].append(exact_match)

            if syntax_valid:
                results['syntax_valid'] += 1
            if has_streamlit:
                results['has_streamlit'] += 1

            # Print metrics for this example
            print(f"  BLEU: {bleu:.3f} | ROUGE-L: {rouge:.3f} | Exact: {exact_match:.1f}")
            print(f"  Syntax: {'ok' if syntax_valid else 'not ok'} | Streamlit: {'ok' if has_streamlit else 'not ok'}")

            results['detailed_results'].append({
                'instruction': instruction,
                'generated': generated_code[:200] + "..." if len(generated_code) > 200 else generated_code,
                'bleu': bleu,
                'rouge': rouge,
                'exact_match': exact_match,
                'syntax_valid': syntax_valid,
                'has_streamlit': has_streamlit
            })

    return results






# Run the evaluation
print(" Starting comprehensive evaluation...")
evaluation_results = run_complete_evaluation()

# Cell 21: Final Evaluation Summary
def print_final_results(results):
    """Print comprehensive evaluation summary"""

    # Calculate averages
    avg_bleu = np.mean(results['bleu_scores'])
    avg_rouge = np.mean(results['rouge_scores'])
    avg_exact_match = np.mean(results['exact_matches'])

    syntax_rate = (results['syntax_valid'] / results['total_examples']) * 100
    streamlit_rate = (results['has_streamlit'] / results['total_examples']) * 100

    print("\n" + "="*60)
    print(" FINAL EVALUATION RESULTS")
    print("="*60)

    print(f"\n Generation Quality Metrics:")
    print(f"   • Average BLEU Score:    {avg_bleu:.3f}")
    print(f"   • Average ROUGE-L:       {avg_rouge:.3f}")
    print(f"   • Exact Match Rate:      {avg_exact_match:.1%}")

    print(f"\n Code Quality Metrics:")
    print(f"   • Syntax Validity:       {syntax_rate:.1f}% ({results['syntax_valid']}/{results['total_examples']})")
    print(f"   • Streamlit Imports:     {streamlit_rate:.1f}% ({results['has_streamlit']}/{results['total_examples']})")

    print(f"\nScore Interpretation:")
    print(f"   • BLEU 0.05+:  Decent overlap with reference")
    print(f"   • ROUGE 0.10+: Good content similarity")
    print(f"   • Syntax 60%+: Acceptable for code generation")

    print(f"\nOverall Assessment:")
    if avg_bleu > 0.05 and syntax_rate > 60:
        print("   Model shows good fine-tuning results")
    elif syntax_rate > 60:
        print("   Model generates valid code but different style")
    else:
        print("   Model needs more training or data")

    return {
        'avg_bleu': avg_bleu,
        'avg_rouge': avg_rouge,
        'syntax_rate': syntax_rate,
        'streamlit_rate': streamlit_rate
    }




#now lets print final results
import numpy as np
final_metrics = print_final_results(evaluation_results)





# Cell 22: Prepare Model for Hugging Face Upload to use it for inference (this will upload the code to your huggingfave account)
from huggingface_hub import HfApi, create_repo
import os

# Check what files we have
print("Files in your fine-tuned model directory:")
model_files = os.listdir("/content/qwen-improved-final")
for file in model_files:
    file_path = os.path.join("/content/qwen-improved-final", file)
    size = os.path.getsize(file_path) / (1024*1024)  # Size in MB
    print(f"   • {file} ({size:.1f} MB)")

print(f"\nTotal files: {len(model_files)}")

# Create a model card (broken into parts to avoid syntax issues)
model_card_content = "---\n"
model_card_content += "license: apache-2.0\n"
model_card_content += "base_model: Qwen/Qwen2.5-Coder-1.5B-Instruct\n"
model_card_content += "tags:\n- code-generation\n- streamlit\n- fine-tuned\n- lora\n"
model_card_content += "language:\n- en\n"
model_card_content += "---\n\n"
model_card_content += "# Qwen2.5-Coder Fine-tuned for Streamlit Code Generation\n\n"
model_card_content += "This model is a LoRA fine-tuned version of Qwen/Qwen2.5-Coder-1.5B-Instruct, specialized for generating Streamlit applications.\n\n"
model_card_content += "## Training Details\n"
model_card_content += "- **Base Model:** Qwen/Qwen2.5-Coder-1.5B-Instruct\n"
model_card_content += "- **Fine-tuning Method:** LoRA (Low-Rank Adaptation)\n"
model_card_content += "- **Training Data:** 48 Streamlit code examples\n"
model_card_content += "- **Training Examples:** 38 train, 10 validation\n\n"
model_card_content += "## Evaluation Results\n"
model_card_content += "- **BLEU Score:** 0.047\n"
model_card_content += "- **ROUGE-L:** 0.119\n"
model_card_content += "- **Syntax Validity:** 60%\n"
model_card_content += "- **Streamlit Import Rate:** 100%\n\n"
model_card_content += "## Model Performance\n"
model_card_content += "Specialized for generating Streamlit applications with proper imports and component usage.\n"

# Save model card
with open("/content/qwen-improved-final/README.md", "w") as f:
    f.write(model_card_content)

print("Ready for upload to Hugging Face Hub in model sec")

# Cell 23: Upload Model to Hugging Face Hub
from huggingface_hub import HfApi, create_repo, upload_folder
import os

# Your HF username and model name
HF_USERNAME = "YOUR HUGGING FACE USER NAME"   # replace this with your own username, to upload it there
MODEL_NAME = "qwen-streamlit-coder"
REPO_ID = f"{HF_USERNAME}/{MODEL_NAME}"

print(f"Uploading model to: https://huggingface.co/{REPO_ID}")

try:
    # Create repository on Hugging Face
    print("Creating repository...")
    create_repo(
        repo_id=REPO_ID,
        repo_type="model",
        private=False,  # Set to True if you want private repo
        exist_ok=True   # Don't error if repo already exists
    )
    print(f"Repository created: {REPO_ID}")

    # Upload all files from your model directory
    print("⬆Uploading model files...")
    upload_folder(
        folder_path="/content/qwen-improved-final",
        repo_id=REPO_ID,
        repo_type="model",
        commit_message="Upload fine-tuned Qwen2.5-Coder for Streamlit generation"
    )

    print("Upload successful!")
    print(f"Your model is now available at: https://huggingface.co/{REPO_ID}")

    # Show what was uploaded
    print(f"\n Uploaded files:")
    for file in os.listdir("/content/qwen-improved-final"):
        print(f"    {file}")

except Exception as e:
    print(f"Upload failed: {e}")
    print("Make sure you're logged in to Hugging Face (run login() from earlier)")











